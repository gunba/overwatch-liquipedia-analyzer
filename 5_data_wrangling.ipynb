{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files: 100%|██████████| 3685/3685 [00:00<00:00, 3973.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Load all JSON files from the tournaments_data folder\n",
    "data_folder = \"tournaments_data\"\n",
    "all_files = [os.path.join(data_folder, f) for f in os.listdir(data_folder) if f.endswith('.json')]\n",
    "\n",
    "# Initialize lists to store extracted data\n",
    "tournament_info_list = []\n",
    "team_members_list = []\n",
    "match_cards_list = []\n",
    "maps_list = []\n",
    "\n",
    "# Helper function to extract map data\n",
    "def extract_map_data(match_id, match_data):\n",
    "    maps = []\n",
    "    for key, value in match_data.items():\n",
    "        if key.startswith('map'):\n",
    "            map_num = key[3:]\n",
    "            map_info = value if isinstance(value, dict) else {}\n",
    "            map_info['map_num'] = map_num\n",
    "            map_info['match_card_id'] = match_id\n",
    "            maps.append(map_info)\n",
    "    return maps\n",
    "\n",
    "# ID counters\n",
    "tournament_id_counter = 1\n",
    "team_id_counter = 1\n",
    "match_id_counter = 1\n",
    "\n",
    "# Process each file with a progress bar\n",
    "for file in tqdm(all_files, desc=\"Processing JSON files\"):\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "        # Extract tournament_info\n",
    "        tournament_info = data.get(\"tournament_info\", [])\n",
    "        if tournament_info:\n",
    "            tournament_info = tournament_info[0]\n",
    "        else:\n",
    "            tournament_info = {}\n",
    "        \n",
    "        tournament_info[\"tournament_id\"] = tournament_id_counter\n",
    "        tournament_info_list.append(tournament_info)\n",
    "        \n",
    "        # Extract team members\n",
    "        team_cards = data.get(\"team_cards\", [])\n",
    "        for team in team_cards:\n",
    "            if not team.get(\"team\"):\n",
    "                continue\n",
    "            team_id = team_id_counter\n",
    "            team_id_counter += 1\n",
    "            members = team.get(\"members\", [])\n",
    "            for member in members:\n",
    "                if not member.get(\"name\"):\n",
    "                    continue\n",
    "                team_member_info = {\n",
    "                    \"team\": team.get(\"team\", \"\"),\n",
    "                    \"tournament_id\": tournament_id_counter,\n",
    "                    \"team_id\": team_id\n",
    "                }\n",
    "                team_member_info.update(member)  # Add all keys from member\n",
    "                team_members_list.append(team_member_info)\n",
    "        \n",
    "        # Extract match cards\n",
    "        match_cards = data.get(\"match_cards\", [])\n",
    "        for match in match_cards:\n",
    "            if not match.get(\"winner\"):\n",
    "                continue\n",
    "            match_id = match_id_counter\n",
    "            match_id_counter += 1\n",
    "            match['tournament_id'] = tournament_id_counter\n",
    "            match['match_card_id'] = match_id\n",
    "            match_cards_list.append(match)\n",
    "            \n",
    "            # Extract maps data from match cards\n",
    "            maps = extract_map_data(match_id, match)\n",
    "            maps_list.extend(maps)\n",
    "    \n",
    "    tournament_id_counter += 1\n",
    "\n",
    "# Convert lists to DataFrames\n",
    "tournament_info_df = pd.DataFrame(tournament_info_list)\n",
    "team_members_df = pd.DataFrame(team_members_list)\n",
    "match_cards_df = pd.DataFrame(match_cards_list)\n",
    "maps_df = pd.DataFrame(maps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorda\\AppData\\Local\\Temp\\ipykernel_17040\\1363133916.py:3: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df.applymap(lambda x: np.nan if isinstance(x, str) and x.strip() == '' else x)\n"
     ]
    }
   ],
   "source": [
    "# Convert all empty whitespace values to NaN\n",
    "def convert_whitespace_to_nan(df):\n",
    "    return df.applymap(lambda x: np.nan if isinstance(x, str) and x.strip() == '' else x)\n",
    "\n",
    "tournament_info_df = convert_whitespace_to_nan(tournament_info_df)\n",
    "team_members_df = convert_whitespace_to_nan(team_members_df)\n",
    "match_cards_df = convert_whitespace_to_nan(match_cards_df)\n",
    "maps_df = convert_whitespace_to_nan(maps_df)\n",
    "\n",
    "# Function to validate date\n",
    "def is_valid_date(date_str):\n",
    "    try:\n",
    "        pd.to_datetime(date_str, format=\"%Y-%m-%d\", errors='raise')\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Function to validate date and convert to datetime\n",
    "def to_datetime(date_str):\n",
    "    try:\n",
    "        return pd.to_datetime(date_str, format=\"%Y-%m-%d\", errors='raise')\n",
    "    except ValueError:\n",
    "        return pd.NaT\n",
    "    \n",
    "# Function to compare dates and return the least recent valid date\n",
    "def get_least_recent_date(dates):\n",
    "    valid_dates = [date for date in dates if pd.notna(to_datetime(date))]\n",
    "    return min(valid_dates) if valid_dates else None\n",
    "\n",
    "# Calculate agg_date\n",
    "tournament_info_df['agg_date'] = tournament_info_df.apply(lambda row: get_least_recent_date(\n",
    "    [row.get('date', ''), row.get('sdate', ''), row.get('edate', '')]\n",
    "), axis=1)\n",
    "\n",
    "# Convert agg_date to just the date part (YYYY-MM-DD)\n",
    "#tournament_info_df['agg_date'] = pd.to_datetime(tournament_info_df['agg_date']).dt.date\n",
    "\n",
    "# Drop rows with no valid date\n",
    "tournament_info_df = tournament_info_df[tournament_info_df['agg_date'].notna()]\n",
    "\n",
    "# Sort by agg_date and create the date_order column\n",
    "tournament_info_df = tournament_info_df.sort_values(by='agg_date').reset_index(drop=True)\n",
    "tournament_info_df['date_order'] = tournament_info_df.index + 1\n",
    "\n",
    "# Drop rows where liquipediatier column is 5.0\n",
    "tournament_info_df = tournament_info_df[tournament_info_df['liquipediatier'] != 5.0]\n",
    "\n",
    "# Keep only specified columns\n",
    "columns_to_keep = ['name', 'series', 'organizer', 'type', 'city', 'country', 'prizepool', 'prizepoolusd', 'format', 'date', 'sdate', 'edate', 'liquipediatier', 'team_number', 'agg_date', 'tournament_id', 'date_order']\n",
    "tournament_info_df = tournament_info_df[columns_to_keep]\n",
    "\n",
    "# Process match_cards_df\n",
    "# Fill missing date values from tournament agg_date\n",
    "match_cards_df = match_cards_df.merge(tournament_info_df[['tournament_id', 'agg_date']], on='tournament_id', how='left', suffixes=('', '_tournament'))\n",
    "match_cards_df['date'] = match_cards_df.apply(lambda row: row['date'] if not pd.isna(row['date']) else row['agg_date'], axis=1)\n",
    "match_cards_df.drop(columns=['agg_date'], inplace=True)\n",
    "\n",
    "# Drop rows without opponent1 or opponent2\n",
    "match_cards_df = match_cards_df.dropna(subset=['opponent1', 'opponent2'])\n",
    "\n",
    "# Drop rows where opponent1 or opponent2 is 'BYE'\n",
    "match_cards_df = match_cards_df[\n",
    "    (match_cards_df['opponent1'] != 'BYE') & (match_cards_df['opponent2'] != 'BYE')\n",
    "]\n",
    "\n",
    "# Keep only specified columns\n",
    "columns_to_keep = ['date', 'date_timezone', 'opponent1', 'opponent2', 'opponent1_score', 'opponent2_score', 'winner', 'format', 'date_time', 'key', 'tournament_id', 'match_card_id']\n",
    "match_cards_df = match_cards_df[columns_to_keep]\n",
    "\n",
    "# Keep only specified columns\n",
    "columns_to_keep = ['map', 'mode', 'score1', 'score2', 'winner', 'map_num', 'match_card_id'] \n",
    "maps_df = maps_df[columns_to_keep]\n",
    "\n",
    "# Drop related entries in other dataframes for removed tournaments\n",
    "valid_tournament_ids = set(tournament_info_df['tournament_id'])\n",
    "\n",
    "# Drop match_cards_df records where tournament_id is not in tournament_id in tournament_info_df\n",
    "match_cards_df = match_cards_df[match_cards_df['tournament_id'].isin(valid_tournament_ids)]\n",
    "\n",
    "# Drop maps where match_card_id is not in match_cards_df.match_card_id\n",
    "valid_match_card_ids = set(match_cards_df['match_card_id'])\n",
    "maps_df = maps_df[maps_df['match_card_id'].isin(valid_match_card_ids)]\n",
    "\n",
    "# Drop teams where tournament_id is not in tournament_info_df\n",
    "team_members_df = team_members_df[team_members_df['tournament_id'].isin(valid_tournament_ids)]\n",
    "\n",
    "# Track if each tournament has players or not for later.\n",
    "tournament_info_df['has_players'] = tournament_info_df['tournament_id'].isin(team_members_df['tournament_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26745/26745 [00:05<00:00, 4582.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 1. Max Index 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12651/12651 [00:12<00:00, 1008.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.98. Max Index 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10172/10172 [00:10<00:00, 986.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.96. Max Index 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10161/10161 [00:10<00:00, 986.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.94. Max Index 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10159/10159 [00:10<00:00, 933.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.9199999999999999. Max Index 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10083/10083 [00:10<00:00, 957.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.8999999999999999. Max Index 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9943/9943 [00:10<00:00, 937.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.8799999999999999. Max Index 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9845/9845 [00:10<00:00, 912.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.8599999999999999. Max Index 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9685/9685 [00:11<00:00, 869.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.8399999999999999. Max Index 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9444/9444 [00:11<00:00, 798.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.8199999999999998. Max Index 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9105/9105 [00:12<00:00, 735.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.7999999999999998. Max Index 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8671/8671 [00:13<00:00, 632.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.7799999999999998. Max Index 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8070/8070 [00:14<00:00, 550.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.7599999999999998. Max Index 210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [00:16<00:00, 449.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.7399999999999998. Max Index 315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7249/7249 [00:36<00:00, 199.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.7199999999999998. Max Index 473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6863/6863 [00:28<00:00, 241.00it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from difflib import get_close_matches\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "\n",
    "# Set up tqdm with pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load the acronym map\n",
    "with open('acronym_map.json', 'r') as file:\n",
    "    acronym_map = json.load(file)\n",
    "\n",
    "# Map opponent acronyms to full names, retain original name if not found\n",
    "def map_acronyms(team_name):\n",
    "    return acronym_map.get(team_name, team_name)\n",
    "\n",
    "match_cards_df['opponent1'] = match_cards_df['opponent1'].apply(map_acronyms)\n",
    "match_cards_df['opponent2'] = match_cards_df['opponent2'].apply(map_acronyms)\n",
    "\n",
    "# Terms to remove\n",
    "terms_to_remove = ['GAMING', 'TEAM', 'ESPORTS', 'ESPORT', 'CLUB']\n",
    "\n",
    "def clean_team_name(name):\n",
    "    # Remove specified terms\n",
    "    for term in terms_to_remove:\n",
    "        name = name.replace(term, '')\n",
    "    # Remove non-alphanumeric characters\n",
    "    name = re.sub(r'\\W+', '', name)\n",
    "    return name.strip().upper()  # Convert to upper case for uniformity\n",
    "\n",
    "# Create clean_team column in team_members_df\n",
    "team_members_df['clean_team'] = team_members_df['team'].apply(clean_team_name)\n",
    "tournament_id_to_date_order = tournament_info_df.set_index('tournament_id')['date_order'].to_dict()\n",
    "tournament_id_to_liquipediatier = tournament_info_df.set_index('tournament_id')['liquipediatier'].to_dict()\n",
    "tournament_id_to_agg_date = tournament_info_df.set_index('tournament_id')['agg_date'].to_dict()\n",
    "\n",
    "# Add date_order, liquipediatier, and agg_date to match_cards_df using the mapping\n",
    "match_cards_df['date_order'] = match_cards_df['tournament_id'].map(tournament_id_to_date_order)\n",
    "match_cards_df['liquipediatier'] = match_cards_df['tournament_id'].map(tournament_id_to_liquipediatier)\n",
    "match_cards_df['agg_date'] = match_cards_df['tournament_id'].map(tournament_id_to_agg_date)\n",
    "\n",
    "# Create unique_teams dataframe directly from team_members_df\n",
    "unique_teams = team_members_df[['clean_team', 'team_id', 'tournament_id']].drop_duplicates()\n",
    "\n",
    "# Add date_order, liquipediatier, and agg_date to unique_teams using the mapping\n",
    "unique_teams['date_order'] = unique_teams['tournament_id'].map(tournament_id_to_date_order)\n",
    "unique_teams['liquipediatier'] = unique_teams['tournament_id'].map(tournament_id_to_liquipediatier)\n",
    "unique_teams['agg_date'] = unique_teams['tournament_id'].map(tournament_id_to_agg_date)\n",
    "\n",
    "# Initial matching within the same tournament\n",
    "def find_closest_team_id_within_tournament(row, unique_teams, threshold=0.8):\n",
    "    tournament_teams = unique_teams[unique_teams['tournament_id'] == row['tournament_id']]\n",
    "    team_names = tournament_teams['clean_team'].tolist()\n",
    "    team_ids = tournament_teams['team_id'].tolist()\n",
    "    \n",
    "    best_team1_id, best_team2_id = None, None\n",
    "    \n",
    "    if team_names:\n",
    "        matches1 = get_close_matches(row['clean_opponent1'], team_names, n=1, cutoff=threshold)\n",
    "        matches2 = get_close_matches(row['clean_opponent2'], team_names, n=1, cutoff=threshold)\n",
    "        \n",
    "        if matches1:\n",
    "            best_team1_id = team_ids[team_names.index(matches1[0])]\n",
    "        if matches2:\n",
    "            best_team2_id = team_ids[team_names.index(matches2[0])]\n",
    "    \n",
    "    return best_team1_id, best_team2_id\n",
    "\n",
    "# Apply initial matching with progress bar\n",
    "match_cards_df['clean_opponent1'] = match_cards_df['opponent1'].apply(clean_team_name)\n",
    "match_cards_df['clean_opponent2'] = match_cards_df['opponent2'].apply(clean_team_name)\n",
    "\n",
    "# Initial matching\n",
    "team_ids = match_cards_df.progress_apply(lambda row: find_closest_team_id_within_tournament(row, unique_teams, 0.8), axis=1)\n",
    "match_cards_df[['team1_id', 'team2_id']] = pd.DataFrame(team_ids.tolist(), index=match_cards_df.index)\n",
    "\n",
    "# Function to find the closest matching team_id with index distance, liquipediatier constraint, and identical agg_date check\n",
    "def find_closest_team_id_with_index_distance(row, unique_teams, threshold=0.8, max_index=1):\n",
    "    date_order = row['date_order']\n",
    "    agg_date = row['agg_date']\n",
    "    tier = row['liquipediatier']\n",
    "    date_range = range(date_order - max_index, date_order + 1)  # Looking backward in time\n",
    "    \n",
    "    candidate_teams = unique_teams[((unique_teams['date_order'].isin(date_range)) | \n",
    "                                   (unique_teams['agg_date'] == agg_date)) & \n",
    "                                   (unique_teams['liquipediatier'].between(tier - 1, tier + 1))]\n",
    "    team_names = candidate_teams['clean_team'].tolist()\n",
    "    team_ids = candidate_teams['team_id'].tolist()\n",
    "    \n",
    "    best_team1_id, best_team2_id = None, None\n",
    "    \n",
    "    if team_names:\n",
    "        matches1 = get_close_matches(row['clean_opponent1'], team_names, n=1, cutoff=threshold)\n",
    "        matches2 = get_close_matches(row['clean_opponent2'], team_names, n=1, cutoff=threshold)\n",
    "        \n",
    "        if matches1:\n",
    "            best_team1_id = team_ids[team_names.index(matches1[0])]\n",
    "        if matches2:\n",
    "            best_team2_id = team_ids[team_names.index(matches2[0])]\n",
    "    \n",
    "    return best_team1_id, best_team2_id\n",
    "\n",
    "# Loop to progressively reduce threshold and increase index distance\n",
    "def progressively_match_teams(match_cards_df, unique_teams):\n",
    "    threshold = 1\n",
    "    max_index = 1\n",
    "\n",
    "    while True:\n",
    "        missing_team1_ids = match_cards_df['team1_id'].isna()\n",
    "        missing_team2_ids = match_cards_df['team2_id'].isna()\n",
    "\n",
    "        if not missing_team1_ids.any() and not missing_team2_ids.any():\n",
    "            break\n",
    "\n",
    "        print(f\"Threshold {threshold}. Max Index {max_index}\")\n",
    "\n",
    "        team_ids = match_cards_df[missing_team1_ids | missing_team2_ids].progress_apply(\n",
    "            lambda row: find_closest_team_id_with_index_distance(row, unique_teams, threshold, max_index),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        match_cards_df.loc[missing_team1_ids, 'team1_id'] = team_ids.apply(lambda x: x[0])\n",
    "        match_cards_df.loc[missing_team2_ids, 'team2_id'] = team_ids.apply(lambda x: x[1])\n",
    "\n",
    "        # Decrease threshold and increase index distance\n",
    "        threshold -= 0.02\n",
    "        max_index = math.ceil(max_index * 1.5)\n",
    "\n",
    "        # Stop if threshold is too low\n",
    "        if threshold < 0.7:\n",
    "            break\n",
    "\n",
    "# Apply the progressive matching\n",
    "progressively_match_teams(match_cards_df, unique_teams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26745 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26745/26745 [05:26<00:00, 81.86it/s] \n"
     ]
    }
   ],
   "source": [
    "# Function to duplicate team members and assign new team IDs\n",
    "def duplicate_team_members(team_members_df, original_team_id, new_team_id):\n",
    "    new_team_members = team_members_df[team_members_df['team_id'] == original_team_id].copy()\n",
    "    new_team_members['team_id'] = new_team_id\n",
    "    return new_team_members\n",
    "\n",
    "# Create a new DataFrame to store duplicated team members\n",
    "duplicated_team_members = pd.DataFrame()\n",
    "\n",
    "# Keep track of the original team IDs that have been duplicated\n",
    "duplicated_team_ids = set()\n",
    "\n",
    "# Get the maximum existing team ID and initialize new_team_id counter\n",
    "max_team_id = team_members_df['team_id'].max()\n",
    "new_team_id_counter = max_team_id + 1\n",
    "\n",
    "# Loop through match_cards_df to create unique team IDs and duplicate team members\n",
    "for index, row in tqdm(match_cards_df.iterrows(), total=match_cards_df.shape[0]):\n",
    "    if not pd.isna(row['team1_id']):\n",
    "        new_team1_id = new_team_id_counter\n",
    "        duplicated_team_members = pd.concat([duplicated_team_members, duplicate_team_members(team_members_df, row['team1_id'], new_team1_id)])\n",
    "        duplicated_team_ids.add(row['team1_id'])\n",
    "        match_cards_df.at[index, 'team1_id'] = new_team1_id\n",
    "        new_team_id_counter += 1\n",
    "    \n",
    "    if not pd.isna(row['team2_id']):\n",
    "        new_team2_id = new_team_id_counter\n",
    "        duplicated_team_members = pd.concat([duplicated_team_members, duplicate_team_members(team_members_df, row['team2_id'], new_team2_id)])\n",
    "        duplicated_team_ids.add(row['team2_id'])\n",
    "        match_cards_df.at[index, 'team2_id'] = new_team2_id\n",
    "        new_team_id_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_members_df = duplicated_team_members\n",
    "\n",
    "# Identify valid team IDs based on the presence of players\n",
    "valid_team_ids = set(team_members_df[team_members_df['role_type'] == 'player']['team_id'].unique())\n",
    "\n",
    "# Set team1_id and team2_id to null if they do not point to any team members with players\n",
    "match_cards_df.loc[~match_cards_df['team1_id'].isin(valid_team_ids), 'team1_id'] = None\n",
    "match_cards_df.loc[~match_cards_df['team2_id'].isin(valid_team_ids), 'team2_id'] = None\n",
    "\n",
    "# Drop match_cards that have null in either team1_id or team2_id\n",
    "match_cards_df.dropna(subset=['team1_id', 'team2_id'], inplace=True)\n",
    "\n",
    "# Filter out team members that are not referenced in match_cards_df\n",
    "used_team_ids = set(match_cards_df['team1_id']).union(set(match_cards_df['team2_id']))\n",
    "team_members_df = team_members_df[team_members_df['team_id'].isin(used_team_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 317206 entries, 6 to 200424\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   team           317206 non-null  object\n",
      " 1   tournament_id  317206 non-null  int64 \n",
      " 2   team_id        317206 non-null  int64 \n",
      " 3   name           317206 non-null  object\n",
      " 4   position       250368 non-null  object\n",
      " 5   flag           171400 non-null  object\n",
      " 6   role_type      317206 non-null  object\n",
      " 7   clean_team     317206 non-null  object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 21.8+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 317206 entries, 6 to 200424\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   team           317206 non-null  object\n",
      " 1   tournament_id  317206 non-null  int64 \n",
      " 2   team_id        317206 non-null  int64 \n",
      " 3   name           317206 non-null  object\n",
      " 4   position       280048 non-null  object\n",
      " 5   flag           277656 non-null  object\n",
      " 6   role_type      317206 non-null  object\n",
      " 7   clean_team     317206 non-null  object\n",
      " 8   unique_id      317206 non-null  int64 \n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 32.3+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorda\\AppData\\Local\\Temp\\ipykernel_17040\\957915810.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  team_members_df[\"unique_id\"] = team_members_df.apply(get_combination_id, axis=1)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Display the initial info of the DataFrame\n",
    "team_members_df.info()\n",
    "\n",
    "# Helper function to find the most common value in a list\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "# Helper function to create lookup dictionary\n",
    "def create_lookup(df, key_cols, value_col):\n",
    "    lookup = defaultdict(list)\n",
    "    for _, row in df[df[value_col].notna()].iterrows():\n",
    "        key = tuple(row[col] for col in key_cols)\n",
    "        lookup[key].append(row[value_col])\n",
    "    return {\n",
    "        key: most_common(values)\n",
    "        for key, values in lookup.items()\n",
    "    }\n",
    "\n",
    "# Impute missing values based on lookup dictionary\n",
    "def impute_missing_values(df, lookup, key_cols, value_col):\n",
    "    for idx, row in df[df[value_col].isna()].iterrows():\n",
    "        key = tuple(row[col] for col in key_cols)\n",
    "        if key in lookup:\n",
    "            df.at[idx, value_col] = lookup[key]\n",
    "\n",
    "# Step 2: Impute missing position values\n",
    "position_lookup = create_lookup(team_members_df, [\"name\", \"flag\", \"role_type\"], \"position\")\n",
    "impute_missing_values(team_members_df, position_lookup, [\"name\", \"flag\", \"role_type\"], \"position\")\n",
    "\n",
    "# Step 3: Impute missing flag values\n",
    "flag_lookup = create_lookup(team_members_df, [\"name\", \"position\", \"role_type\"], \"flag\")\n",
    "impute_missing_values(team_members_df, flag_lookup, [\"name\", \"position\", \"role_type\"], \"flag\")\n",
    "\n",
    "# Step 4: Impute remaining values based on name and role_type alone\n",
    "position_lookup_name = create_lookup(team_members_df, [\"name\", \"role_type\"], \"position\")\n",
    "impute_missing_values(team_members_df, position_lookup_name, [\"name\", \"role_type\"], \"position\")\n",
    "\n",
    "flag_lookup_name = create_lookup(team_members_df, [\"name\", \"role_type\"], \"flag\")\n",
    "impute_missing_values(team_members_df, flag_lookup_name, [\"name\", \"role_type\"], \"flag\")\n",
    "\n",
    "# Step 6: Create a unique ID for combinations of NAME, FLAG, POSITION, and ROLE_TYPE\n",
    "combination_to_id = {}\n",
    "current_id = 1\n",
    "\n",
    "def get_combination_id(row):\n",
    "    global current_id\n",
    "    if row[\"role_type\"] == 'player':\n",
    "        key = (\n",
    "            row[\"name\"],\n",
    "            row[\"flag\"] if pd.notna(row[\"flag\"]) else None,\n",
    "            row[\"position\"] if pd.notna(row[\"position\"]) else None\n",
    "        )\n",
    "    else:\n",
    "        key = (\n",
    "            row[\"name\"],\n",
    "            row[\"flag\"] if pd.notna(row[\"flag\"]) else None,\n",
    "            row[\"role_type\"]\n",
    "        )\n",
    "        \n",
    "    if key not in combination_to_id:\n",
    "        combination_to_id[key] = current_id\n",
    "        current_id += 1\n",
    "        \n",
    "    return combination_to_id[key]\n",
    "\n",
    "team_members_df[\"unique_id\"] = team_members_df.apply(get_combination_id, axis=1)\n",
    "\n",
    "# Display the updated DataFrame info\n",
    "team_members_df.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorda\\AppData\\Local\\Temp\\ipykernel_17040\\4163476928.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  members_df['elo_before'] = initial_elo\n",
      "C:\\Users\\jorda\\AppData\\Local\\Temp\\ipykernel_17040\\4163476928.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  members_df['elo_after'] = initial_elo\n",
      "C:\\Users\\jorda\\AppData\\Local\\Temp\\ipykernel_17040\\4163476928.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  members_df['c_elo_before'] = initial_elo\n",
      "C:\\Users\\jorda\\AppData\\Local\\Temp\\ipykernel_17040\\4163476928.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  members_df['c_elo_after'] = initial_elo\n",
      "Processing matches: 100%|██████████| 20339/20339 [01:14<00:00, 273.79it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration parameters\n",
    "config = {\n",
    "    \"base_k\": 50,  # Base K-factor for ELO calculations\n",
    "    \"initial_elo\": 1500.0,  # Initial ELO rating for all players and coaches\n",
    "    \"tier_multiplier_factor_pre_2024\": 12,  # Pre-2024 tier multiplier factor\n",
    "    \"tier_exponent_pre_2024\": 2,  # Pre-2024 tier exponent\n",
    "    \"tier_multiplier_factor_post_2024\": 6,  # Post-2024 tier multiplier factor\n",
    "    \"tier_exponent_post_2024\": 1,  # Post-2024 tier exponent\n",
    "}\n",
    "\n",
    "# There will be duplicate indexes from data duplication step. We use indexes to speed up this step 4x.\n",
    "team_members_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def calculate_elo_change_common(player_avg_elo, opponent_avg_elo, outcome, base_k, tier_multiplier):\n",
    "    expected_score = 1 / (1 + 10 ** ((opponent_avg_elo - player_avg_elo) / 400))\n",
    "    change = (base_k * tier_multiplier) * (outcome - expected_score)\n",
    "    return change, expected_score\n",
    "\n",
    "def create_team_member_lookup(members_df):\n",
    "    # Create a dictionary to map team_id to player and coach indices\n",
    "    team_member_lookup = {}\n",
    "    \n",
    "    for idx, row in members_df.iterrows():\n",
    "        team_id = row[\"team_id\"]\n",
    "        role_type = row[\"role_type\"]\n",
    "        position = row.get(\"position\", \"\")\n",
    "\n",
    "        if team_id not in team_member_lookup:\n",
    "            team_member_lookup[team_id] = {\n",
    "                \"players\": [],\n",
    "                \"all_members\": []\n",
    "            }\n",
    "        \n",
    "        if role_type == 'player':\n",
    "            team_member_lookup[team_id][\"players\"].append(idx)\n",
    "        if role_type == 'player' or (pd.notna(position) and 'COACH' in position):\n",
    "            team_member_lookup[team_id][\"all_members\"].append(idx)\n",
    "    \n",
    "    return team_member_lookup\n",
    "\n",
    "def update_elo_ratings(matches_df, members_df, config):\n",
    "    base_k = config[\"base_k\"]\n",
    "    initial_elo = config[\"initial_elo\"]\n",
    "    tier_multiplier_factor_pre_2024 = config[\"tier_multiplier_factor_pre_2024\"]\n",
    "    tier_exponent_pre_2024 = config[\"tier_exponent_pre_2024\"]\n",
    "    tier_multiplier_factor_post_2024 = config[\"tier_multiplier_factor_post_2024\"]\n",
    "    tier_exponent_post_2024 = config[\"tier_exponent_post_2024\"]\n",
    "\n",
    "    # Sort data outside the loop\n",
    "    matches_df.sort_values(by='date', inplace=True)\n",
    "\n",
    "    # Initialize ELO ratings\n",
    "    members_df['elo_before'] = initial_elo\n",
    "    members_df['elo_after'] = initial_elo\n",
    "    members_df['c_elo_before'] = initial_elo\n",
    "    members_df['c_elo_after'] = initial_elo\n",
    "    elo_ratings = {}\n",
    "    coach_elo_ratings = {}\n",
    "\n",
    "    # Drop columns if they already exist\n",
    "    for col in ['team1_avg_elo', 'team2_avg_elo', 'team1_expected_outcome', 'team2_expected_outcome']:\n",
    "        if col in matches_df.columns:\n",
    "            matches_df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    # Initial values for tier multipliers\n",
    "    tier_multiplier_factor = tier_multiplier_factor_pre_2024\n",
    "    tier_exponent = tier_exponent_pre_2024\n",
    "\n",
    "    # Create a lookup table for team members\n",
    "    team_member_lookup = create_team_member_lookup(members_df)\n",
    "\n",
    "    for _, match in tqdm(matches_df.iterrows(), total=len(matches_df), desc=\"Processing matches\"):\n",
    "        # Pre-fetch match values\n",
    "        match_id = match[\"match_card_id\"]\n",
    "        date = match[\"date\"]\n",
    "        winner = match[\"winner\"]\n",
    "        team1_id = match[\"team1_id\"]\n",
    "        team2_id = match[\"team2_id\"]\n",
    "        tier = match[\"liquipediatier\"]\n",
    "\n",
    "        # Ensure both team1_id and team2_id are present\n",
    "        if pd.isna(team1_id) or pd.isna(team2_id):\n",
    "            continue\n",
    "        \n",
    "        # Get team player indices once and reuse\n",
    "        team1_player_indices = team_member_lookup.get(team1_id, {}).get(\"players\", [])\n",
    "        team2_player_indices = team_member_lookup.get(team2_id, {}).get(\"players\", [])\n",
    "        \n",
    "        # Get all member indices (players and staff with 'COACH' in their position)\n",
    "        team1_all_member_indices = team_member_lookup.get(team1_id, {}).get(\"all_members\", [])\n",
    "        team2_all_member_indices = team_member_lookup.get(team2_id, {}).get(\"all_members\", [])\n",
    "\n",
    "        # Skip matches where either team has no players\n",
    "        if not team1_player_indices or not team2_player_indices:\n",
    "            continue\n",
    "\n",
    "        # Update tier multipliers if the date threshold is crossed\n",
    "        if date >= \"2024-01-01\" and (tier_multiplier_factor != tier_multiplier_factor_post_2024 or tier_exponent != tier_exponent_post_2024):\n",
    "            tier_multiplier_factor = tier_multiplier_factor_post_2024\n",
    "            tier_exponent = tier_exponent_post_2024\n",
    "        \n",
    "        if winner not in [1, 2]:\n",
    "            continue\n",
    "        \n",
    "        tier_multiplier = tier_multiplier_factor / ((tier + 1) ** tier_exponent)\n",
    "        \n",
    "        # Retrieve and set ELOs for team members before the match\n",
    "        for idx in team1_player_indices + team2_player_indices:\n",
    "            unique_id = members_df.at[idx, 'unique_id']\n",
    "            if unique_id not in elo_ratings:\n",
    "                elo_ratings[unique_id] = initial_elo\n",
    "        \n",
    "        for idx in team1_all_member_indices + team2_all_member_indices:\n",
    "            unique_id = members_df.at[idx, 'unique_id']\n",
    "            if unique_id not in coach_elo_ratings:\n",
    "                coach_elo_ratings[unique_id] = initial_elo\n",
    "\n",
    "        team1_avg_elo = sum(elo_ratings[members_df.at[idx, 'unique_id']] for idx in team1_player_indices) / len(team1_player_indices)\n",
    "        team2_avg_elo = sum(elo_ratings[members_df.at[idx, 'unique_id']] for idx in team2_player_indices) / len(team2_player_indices)\n",
    "        team1_avg_coach_elo = sum(coach_elo_ratings[members_df.at[idx, 'unique_id']] for idx in team1_all_member_indices) / len(team1_all_member_indices)\n",
    "        team2_avg_coach_elo = sum(coach_elo_ratings[members_df.at[idx, 'unique_id']] for idx in team2_all_member_indices) / len(team2_all_member_indices)\n",
    "\n",
    "        outcome1 = 1 if winner == 1 else 0\n",
    "\n",
    "        team1_common_change, team1_expected_outcome = calculate_elo_change_common(team1_avg_elo, team2_avg_elo, outcome1, base_k, tier_multiplier)\n",
    "        team2_common_change = -team1_common_change  # The change for team 2 is the inverse of team 1\n",
    "\n",
    "        team1_coach_change, _ = calculate_elo_change_common(team1_avg_coach_elo, team2_avg_coach_elo, outcome1, base_k, tier_multiplier)\n",
    "        team2_coach_change = -team1_coach_change  # The change for team 2 is the inverse of team 1\n",
    "\n",
    "        # Store the average ELOs and expected outcomes in matches_df\n",
    "        matches_df.loc[matches_df['match_card_id'] == match_id, 'team1_avg_elo'] = team1_avg_elo\n",
    "        matches_df.loc[matches_df['match_card_id'] == match_id, 'team2_avg_elo'] = team2_avg_elo\n",
    "        matches_df.loc[matches_df['match_card_id'] == match_id, 'team1_expected_outcome'] = team1_expected_outcome\n",
    "        matches_df.loc[matches_df['match_card_id'] == match_id, 'team2_expected_outcome'] = 1 - team1_expected_outcome\n",
    "\n",
    "        # Update ELOs for players and coaches using precomputed indices\n",
    "        for team, team_common_change, team_coach_change, team_player_indices, team_all_member_indices in [\n",
    "            (team1_id, team1_common_change, team1_coach_change, team1_player_indices, team1_all_member_indices),\n",
    "            (team2_id, team2_common_change, team2_coach_change, team2_player_indices, team2_all_member_indices)\n",
    "        ]:\n",
    "            for idx in team_player_indices:\n",
    "                unique_id = members_df.at[idx, 'unique_id']\n",
    "                player_elo_before = elo_ratings[unique_id]\n",
    "                player_elo_after = player_elo_before + team_common_change\n",
    "                elo_ratings[unique_id] = player_elo_after\n",
    "\n",
    "                # Update the elo_before and elo_after columns for each match\n",
    "                members_df.at[idx, 'elo_before'] = player_elo_before\n",
    "                members_df.at[idx, 'elo_after'] = player_elo_after\n",
    "\n",
    "            for idx in team_all_member_indices:\n",
    "                unique_id = members_df.at[idx, 'unique_id']\n",
    "                coach_elo_before = coach_elo_ratings[unique_id]\n",
    "                coach_elo_after = coach_elo_before + team_coach_change\n",
    "                coach_elo_ratings[unique_id] = coach_elo_after\n",
    "\n",
    "                # Update the c_elo_before and c_elo_after columns for each match\n",
    "                members_df.at[idx, 'c_elo_before'] = coach_elo_before\n",
    "                members_df.at[idx, 'c_elo_after'] = coach_elo_after\n",
    "\n",
    "# Usage example with match_cards_df, team_members_df, and tournament_info_df already loaded\n",
    "update_elo_ratings(match_cards_df, team_members_df, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 map     mode  score1  score2  winner map_num  match_card_id\n",
      "0         KING'S ROW   HYBRID     3.0     4.0     2.0       1             16\n",
      "1              NEPAL  CONTROL     3.0     0.0     1.0       2             16\n",
      "2             DORADO   ESCORT     2.0     3.0     2.0       3             16\n",
      "3         KING'S ROW   HYBRID     3.0     0.0     1.0       1             17\n",
      "4      LIJIANG TOWER  CONTROL     3.0     1.0     1.0       2             17\n",
      "...              ...      ...     ...     ...     ...     ...            ...\n",
      "14744     KING'S ROW   HYBRID     1.0     2.0     2.0       1          47257\n",
      "14745       ROUTE 66   ESCORT     1.0     2.0     2.0       2          47257\n",
      "14746  LIJIANG TOWER  CONTROL     0.0     2.0     2.0       1          47258\n",
      "14747     KING'S ROW   HYBRID     0.0     1.0     2.0       2          47258\n",
      "14748  CIRCUIT ROYAL   ESCORT     0.0     1.0     2.0       3          47258\n",
      "\n",
      "[14749 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Assuming you have a DataFrame 'maps_df' with your data\n",
    "\n",
    "# Step 1: Define valid map and mode combinations\n",
    "valid_combinations = {\n",
    "    \"KING'S ROW\": 'HYBRID',\n",
    "    'NEPAL': 'CONTROL',\n",
    "    'DORADO': 'ESCORT',\n",
    "    'LIJIANG TOWER': 'CONTROL',\n",
    "    'WATCHPOINT: GIBRALTAR': 'ESCORT',\n",
    "    'NUMBANI': 'HYBRID',\n",
    "    'HANAMURA': 'ASSAULT',\n",
    "    'ILIOS': 'CONTROL',\n",
    "    'TEMPLE OF ANUBIS': 'ASSAULT',\n",
    "    'HOLLYWOOD': 'HYBRID',\n",
    "    'NEW QUEEN STREET': 'PUSH',\n",
    "    'ESPERANÇA': 'PUSH',\n",
    "    'CIRCUIT ROYAL': 'ESCORT',\n",
    "    'ROUTE 66': 'ESCORT',\n",
    "    'EICHENWALDE': 'HYBRID',\n",
    "    'VOLSKAYA INDUSTRIES': 'ASSAULT',\n",
    "    'OASIS': 'CONTROL',\n",
    "    'JUNKERTOWN': 'ESCORT',\n",
    "    'HORIZON LUNAR COLONY': 'ASSAULT',\n",
    "    'COLOSSEO': 'PUSH',\n",
    "    'SURAVASA': 'FLASHPOINT',\n",
    "    'MIDTOWN': 'HYBRID',\n",
    "    'ANTARCTIC PENINSULA': 'CONTROL',\n",
    "    'NEW JUNK CITY': 'FLASHPOINT',\n",
    "    'SAMOA': 'CONTROL',\n",
    "    'RIALTO': 'ESCORT',\n",
    "    'BLIZZARD WORLD': 'HYBRID',\n",
    "    'HAVANA': 'ESCORT',\n",
    "    'BUSAN': 'CONTROL',\n",
    "    'SHAMBALI MONASTERY': 'ESCORT',\n",
    "    'PARAÍSO': 'HYBRID',\n",
    "    'PARIS': 'ASSAULT'\n",
    "}\n",
    "\n",
    "# Convert the valid combinations dictionary to DataFrames for easier manipulation\n",
    "valid_maps_df = pd.DataFrame(valid_combinations.keys(), columns=['map'])\n",
    "valid_modes_df = pd.DataFrame(valid_combinations.values(), columns=['mode'])\n",
    "\n",
    "# Step 2: Drop rows where 'map' or 'mode' is NaN\n",
    "maps_df.dropna(subset=['map', 'mode'], inplace=True)\n",
    "\n",
    "# Step 3: Fuzzy match invalid map names to the closest valid map names\n",
    "def match_map(name, valid_names):\n",
    "    result = process.extractOne(name, valid_names)\n",
    "    return result[0] if result and result[1] >= 60 else name\n",
    "\n",
    "maps_df['map'] = maps_df['map'].apply(lambda x: match_map(x, valid_maps_df['map']))\n",
    "\n",
    "# Step 4: Fuzzy match invalid mode names to the closest valid mode names\n",
    "def match_mode(name, valid_names):\n",
    "    result = process.extractOne(name, valid_names)\n",
    "    return result[0] if result and result[1] >= 60 else name\n",
    "\n",
    "maps_df['mode'] = maps_df['mode'].apply(lambda x: match_mode(x, valid_modes_df['mode']))\n",
    "\n",
    "# Step 5: Filter based on valid combinations\n",
    "valid_combinations_df = pd.DataFrame(list(valid_combinations.items()), columns=['map', 'mode'])\n",
    "maps_df = maps_df.merge(valid_combinations_df, on=['map', 'mode'], how='inner')\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(maps_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to determine Overwatch version\n",
    "def determine_overwatch_version(row, key):\n",
    "    if row[key] >= '2022-10-04':\n",
    "        return 2\n",
    "    elif row['liquipediatier'] == 1 and row[key] >= '2022-05-05':\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Apply the function to both DataFrames\n",
    "tournament_info_df['version'] = tournament_info_df.apply(determine_overwatch_version, axis=1, key='agg_date')\n",
    "match_cards_df['version'] = match_cards_df.apply(determine_overwatch_version, axis=1, key='date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 map     mode  score1  score2  winner map_num  match_card_id\n",
      "0         KING'S ROW   HYBRID     3.0     4.0     2.0       1             16\n",
      "1              NEPAL  CONTROL     3.0     0.0     1.0       2             16\n",
      "2             DORADO   ESCORT     2.0     3.0     2.0       3             16\n",
      "3         KING'S ROW   HYBRID     3.0     0.0     1.0       1             17\n",
      "4      LIJIANG TOWER  CONTROL     3.0     1.0     1.0       2             17\n",
      "...              ...      ...     ...     ...     ...     ...            ...\n",
      "14744     KING'S ROW   HYBRID     1.0     2.0     2.0       1          47257\n",
      "14745       ROUTE 66   ESCORT     1.0     2.0     2.0       2          47257\n",
      "14746  LIJIANG TOWER  CONTROL     0.0     2.0     2.0       1          47258\n",
      "14747     KING'S ROW   HYBRID     0.0     1.0     2.0       2          47258\n",
      "14748  CIRCUIT ROYAL   ESCORT     0.0     1.0     2.0       3          47258\n",
      "\n",
      "[14749 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from collections import Counter\n",
    "\n",
    "# Create a dictionary to store the most common mode for each map\n",
    "most_common_mode = {}\n",
    "\n",
    "# Group by map and find the most common mode for each map\n",
    "map_mode_groups = maps_df.groupby('map')['mode'].apply(lambda x: Counter(x).most_common(1)[0][0])\n",
    "\n",
    "# Update the most_common_mode dictionary with the result\n",
    "most_common_mode.update(map_mode_groups.to_dict())\n",
    "\n",
    "# Apply the most common mode to each map in the DataFrame\n",
    "maps_df['mode'] = maps_df['map'].map(most_common_mode)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(maps_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorda\\AppData\\Local\\Temp\\ipykernel_17040\\1015975562.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  team_members_df['core_id'] = None\n",
      "Processing matches: 100%|██████████| 20339/20339 [01:21<00:00, 248.55it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize variables\n",
    "next_core_id = 1\n",
    "core_rosters = {}\n",
    "\n",
    "# Function to find a matching core with 60% similarity\n",
    "def find_matching_core(new_roster, core_rosters):\n",
    "    for core_id, roster in core_rosters.items():\n",
    "        intersection = set(new_roster).intersection(set(roster))\n",
    "        if len(intersection) / len(roster) > 0.6:\n",
    "            return core_id\n",
    "    return None\n",
    "\n",
    "# Create a new column for core_id\n",
    "team_members_df['core_id'] = None\n",
    "\n",
    "# Sort match_cards_df by match_date\n",
    "match_cards_df = match_cards_df.sort_values(by='date')\n",
    "\n",
    "# Create a lookup table for team players\n",
    "team_player_lookup = {}\n",
    "for idx, row in team_members_df[team_members_df['role_type'] == 'player'].iterrows():\n",
    "    team_id = row[\"team_id\"]\n",
    "    unique_id = row[\"unique_id\"]\n",
    "\n",
    "    if team_id not in team_player_lookup:\n",
    "        team_player_lookup[team_id] = []\n",
    "\n",
    "    team_player_lookup[team_id].append((unique_id, idx))\n",
    "\n",
    "# Process each match in order with tqdm progress bar\n",
    "for _, match_row in tqdm(match_cards_df.iterrows(), total=match_cards_df.shape[0], desc=\"Processing matches\"):\n",
    "    team_ids = [match_row['team1_id'], match_row['team2_id']]\n",
    "    \n",
    "    for team_id in team_ids:\n",
    "        if pd.isna(team_id) or team_id not in team_player_lookup:\n",
    "            continue\n",
    "        \n",
    "        # Get the set of players for the current team_id\n",
    "        current_roster = [uid for uid, idx in team_player_lookup[team_id]]\n",
    "\n",
    "        if len(current_roster) == 0:\n",
    "            continue  # Skip if the roster is empty\n",
    "\n",
    "        # Find a matching core\n",
    "        core_id = find_matching_core(current_roster, core_rosters)\n",
    "        \n",
    "        if core_id is None:\n",
    "            # If no matching core is found, create a new core_id\n",
    "            core_id = next_core_id\n",
    "            next_core_id += 1\n",
    "            core_rosters[core_id] = current_roster\n",
    "        \n",
    "        # Assign the core_id to the current team's players\n",
    "        for _, idx in team_player_lookup[team_id]:\n",
    "            team_members_df.at[idx, 'core_id'] = core_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      core_id                                               name  match_count\n",
      "0           1  [MILO, SEAGULL, NUMLOCKED, OPLAID, DUMMY, CLOC...          967\n",
      "1           2         [DOT, B1AM, VORPAL, NBLITZ, SPOH, EMPERIC]          835\n",
      "2           3      [DAFOX, TVIQ, RENBOT, DEZU, KUDOCHOP, FAVION]          818\n",
      "3           4      [REAVER, KYKY, ADAM, GREGO, SUREFOUR, DEBETT]          766\n",
      "4           5  [MTG, KRAWNNIC, NAPTIME, MANGACHU, DUMPTRUCK, ...          764\n",
      "...       ...                                                ...          ...\n",
      "6490     6491       [YLDA, YUVAI, SA7AYB, SHAMMA, JAY, FNO, MAY]            2\n",
      "6491     6492        [BEAU, DONUT, RURY, KDO, WESAL, POWA, RUDY]            2\n",
      "6492     6493  [SENO, MARO, MYTHICAL, MAISA, LOTUS, ADRENALINDA]            1\n",
      "6493     6494  [NADEN, YARAEN, BLANK, LUND, PASTA, RASEEL, CA...            1\n",
      "6494     6495  [BAZZANELLA, JIRAIYA, LONELY, MEIASLUPPO, JEFE...            1\n",
      "\n",
      "[6495 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert the core_id column to integer\n",
    "#Wteam_members_df['core_id'] = team_members_df['core_id'].astype(int)\n",
    "\n",
    "# Group by core_id to count the number of matches each core has played\n",
    "core_match_counts = team_members_df.groupby('core_id').size().reset_index(name='match_count')\n",
    "\n",
    "# Get the top core rosters by the number of matches played\n",
    "top_core_rosters = core_match_counts.sort_values(by='match_count', ascending=False)\n",
    "\n",
    "# Merge to get player names for each core_id\n",
    "merged_df = pd.merge(team_members_df, top_core_rosters, on='core_id')\n",
    "\n",
    "# Group by core_id and aggregate unique player names\n",
    "core_roster_with_names = merged_df.groupby('core_id')['name'].apply(lambda x: list(set(x))).reset_index()\n",
    "core_roster_with_names['match_count'] = top_core_rosters['match_count'].values\n",
    "\n",
    "# Sort by match_count to show the most matches played\n",
    "core_roster_with_names = core_roster_with_names.sort_values(by='match_count', ascending=False)\n",
    "\n",
    "# Display the result\n",
    "print(core_roster_with_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame as a pickle file\n",
    "tournament_info_df.to_pickle('tournament_info_df.pkl')\n",
    "match_cards_df.to_pickle('match_cards_df.pkl')\n",
    "maps_df.to_pickle('maps_df.pkl')\n",
    "team_members_df.to_pickle('team_members_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create a connection to the SQLite database\n",
    "conn = sqlite3.connect('tournaments.db')\n",
    "\n",
    "# Create cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create tables\n",
    "cur.execute('''\n",
    "CREATE TABLE IF NOT EXISTS tournament_info (\n",
    "    tournament_id INTEGER PRIMARY KEY,\n",
    "    name TEXT,\n",
    "    series TEXT,\n",
    "    organizer TEXT,\n",
    "    type TEXT,\n",
    "    city TEXT,\n",
    "    country TEXT,\n",
    "    prizepool TEXT,\n",
    "    format TEXT,\n",
    "    date TEXT,\n",
    "    sdate TEXT,\n",
    "    edate TEXT,\n",
    "    team_number INTEGER,\n",
    "    agg_date TEXT,\n",
    "    liquipediatier FLOAT,\n",
    "    date_order INTEGER,\n",
    "    has_players BOOLEAN,\n",
    "    version INTEGER\n",
    ")\n",
    "''')\n",
    "\n",
    "cur.execute('''\n",
    "CREATE TABLE IF NOT EXISTS match_cards (\n",
    "    match_card_id INTEGER PRIMARY KEY,\n",
    "    tournament_id INTEGER,\n",
    "    date TEXT,\n",
    "    date_timezone TEXT,\n",
    "    opponent1 TEXT,\n",
    "    opponent2 TEXT,\n",
    "    format TEXT,\n",
    "    date_time TEXT,\n",
    "    key TEXT,\n",
    "    agg_date TEXT,\n",
    "    clean_opponent1 TEXT,\n",
    "    clean_opponent2 TEXT,\n",
    "    opponent1_score INTEGER,\n",
    "    opponent2_score INTEGER,\n",
    "    winner INTEGER,\n",
    "    date_order INTEGER,\n",
    "    liquipediatier FLOAT,\n",
    "    team1_id FLOAT,\n",
    "    team2_id FLOAT,\n",
    "    team1_avg_elo FLOAT,\n",
    "    team2_avg_elo FLOAT,\n",
    "    team1_expected_outcome FLOAT,\n",
    "    team2_expected_outcome FLOAT,\n",
    "    version INTEGER,\n",
    "    FOREIGN KEY(tournament_id) REFERENCES tournament_info(tournament_id)\n",
    ")\n",
    "''')\n",
    "\n",
    "cur.execute('''\n",
    "CREATE TABLE IF NOT EXISTS maps (\n",
    "    match_card_id INTEGER,\n",
    "    map TEXT,\n",
    "    mode TEXT,\n",
    "    map_num TEXT,\n",
    "    score1 FLOAT,\n",
    "    score2 FLOAT,\n",
    "    winner FLOAT,\n",
    "    FOREIGN KEY(match_card_id) REFERENCES match_cards(match_card_id)\n",
    ")\n",
    "''')\n",
    "\n",
    "cur.execute('''\n",
    "CREATE TABLE IF NOT EXISTS team_members (\n",
    "    unique_id INTEGER,\n",
    "    team_id INTEGER,\n",
    "    tournament_id INTEGER,\n",
    "    name TEXT,\n",
    "    position TEXT,\n",
    "    flag TEXT,\n",
    "    role_type TEXT,\n",
    "    clean_team TEXT,\n",
    "    elo_before FLOAT,\n",
    "    elo_after FLOAT,\n",
    "    c_elo_before FLOAT,\n",
    "    c_elo_after FLOAT,\n",
    "    core_id INTEGER,\n",
    "    FOREIGN KEY(team_id) REFERENCES match_cards(team1_id),\n",
    "    FOREIGN KEY(team_id) REFERENCES match_cards(team2_id),\n",
    "    FOREIGN KEY(tournament_id) REFERENCES tournament_info(tournament_id)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insert data into the tables, replacing existing tables if they already exist\n",
    "tournament_info_df.to_sql('tournament_info', conn, if_exists='replace', index=False)\n",
    "match_cards_df.to_sql('match_cards', conn, if_exists='replace', index=False)\n",
    "maps_df.to_sql('maps', conn, if_exists='replace', index=False)\n",
    "team_members_df.to_sql('team_members', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Create indexes\n",
    "cur.execute('CREATE INDEX IF NOT EXISTS idx_team_members_team_id ON team_members(team_id);')\n",
    "cur.execute('CREATE INDEX IF NOT EXISTS idx_match_cards_team1_id ON match_cards(team1_id);')\n",
    "cur.execute('CREATE INDEX IF NOT EXISTS idx_match_cards_team2_id ON match_cards(team2_id);')\n",
    "cur.execute('CREATE INDEX IF NOT EXISTS idx_match_cards_match_card_id ON match_cards(match_card_id);')\n",
    "cur.execute('CREATE INDEX IF NOT EXISTS idx_maps_match_card_id ON maps(match_card_id);')\n",
    "\n",
    "# Commit changes and close connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
